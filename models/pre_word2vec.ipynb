{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ir_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctkj5UAjSZj5",
        "outputId": "445b2360-f680-431c-bac0-fa026222f15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ir_datasets\n",
            "  Downloading ir_datasets-0.5.4-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.5/311.5 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting warc3-wet-clueweb09>=0.2.5\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting trec-car-tools>=2.5.4\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (4.9.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (2.25.1)\n",
            "Collecting zlib-state>=0.1.3\n",
            "  Downloading zlib_state-0.1.5-cp39-cp39-manylinux2010_x86_64.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (4.65.0)\n",
            "Collecting ijson>=3.1.3\n",
            "  Downloading ijson-3.2.0.post0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyautocorpus>=0.1.1\n",
            "  Downloading pyautocorpus-0.1.9-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (4.6.3)\n",
            "Collecting lz4>=3.1.1\n",
            "  Downloading lz4-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unlzw3>=0.2.1\n",
            "  Downloading unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (1.22.4)\n",
            "Collecting warc3-wet>=0.2.3\n",
            "  Downloading warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (6.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (1.26.14)\n",
            "Collecting cbor>=1.0.0\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18920 sha256=204022a311ea96844ec75b259ef7bbc0b5b0e7a7ea059cce7dfa97547dbb4399\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/22/ed/a11944d7fdf4e94c4206a3f760d385122a4d34d8acc12f71a3\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp39-cp39-linux_x86_64.whl size=57288 sha256=43ec43faae98dba54628b02123dac6f343bc6d4d0b47a01d5864169b1c521246\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/10/03/a281e0682ddd4b310431fb25d1a4f53987105267cf46c417f3\n",
            "Successfully built warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, trec-car-tools, pyautocorpus, lz4, ir_datasets\n",
            "Successfully installed cbor-1.0.0 ijson-3.2.0.post0 ir_datasets-0.5.4 lz4-4.3.2 pyautocorpus-0.1.9 trec-car-tools-2.6 unlzw3-0.2.2 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ir_datasets\n",
        "dataset = ir_datasets.load('cranfield')"
      ],
      "metadata": {
        "id": "cMyjjfLxScxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "# Download necessary nltk resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N1kVbnASgQd",
        "outputId": "2646063b-9476-4951-e040-c50ac8970618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, remove_stopwords: bool):\n",
        "    # Convert text to lower case\n",
        "    text = text.lower()\n",
        "    text = ''.join(c for c in text if c.isalnum() or c.isspace())\n",
        "\n",
        "    # Remove punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "    # Stem tokens\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    #preprocessed_text = ' '.join(stemmed_tokens)\n",
        "\n",
        "    return stemmed_tokens"
      ],
      "metadata": {
        "id": "07h6tGlISlav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_docs = pd.DataFrame(columns=['doc_id', 'processed_text'])\n",
        "df_q = pd.DataFrame(columns=['q_id', 'processed_text'])\n",
        "for item in dataset.docs_iter():\n",
        "  proc1 = preprocess_text(item[1], remove_stopwords=True)\n",
        "  proc2 = preprocess_text(item[2], remove_stopwords=True)\n",
        "  #df.loc[len(df.index)] = [item[0], [preprocess_text(item[1], remove_stopwords=True), preprocess_text(item[2], remove_stopwords=True)]] \n",
        "  proc1.extend(proc2)\n",
        "  df_docs.loc[len(df_docs.index)] = [item[0], proc1]\n",
        "for item in dataset.queries_iter():\n",
        "  df_q.loc[len(df_q.index)] = [item[0], preprocess_text(item[1], remove_stopwords=True)] \n",
        "result = df_q.head(10)\n",
        "print(\"First 10 rows of the DataFrame:\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob5olarOSrXC",
        "outputId": "6cfcbf58-89ae-4daf-bc0e-28b2730db2b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] [starting] http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
            "[INFO] [finished] http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz: [00:00] [507kB] [1.10MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 rows of the DataFrame:\n",
            "  q_id                                     processed_text\n",
            "0    1  [similar, law, must, obey, construct, aeroelas...\n",
            "1    2  [structur, aeroelast, problem, associ, flight,...\n",
            "2    4  [problem, heat, conduct, composit, slab, solv,...\n",
            "3    8  [criterion, develop, show, empir, valid, flow,...\n",
            "4    9  [chemic, kinet, system, applic, hyperson, aero...\n",
            "5   10  [theoret, experiment, guid, turbul, couett, fl...\n",
            "6   12  [possibl, relat, avail, pressur, distribut, og...\n",
            "7   13  [method, dash, exact, approxim, dash, present,...\n",
            "8   15  [paper, intern, slip, flow, heat, transfer, st...\n",
            "9   18  [realga, transport, properti, air, avail, wide...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts1 = df_docs.processed_text.tolist()\n",
        "texts2 = df_q.processed_text.tolist()\n",
        "texts1.extend(texts2)"
      ],
      "metadata": {
        "id": "6Vgtk1xRSwuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd-_Z6SHKFiO",
        "outputId": "60f2c10f-6650-43be-9529-ffd3df78e1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sister\n",
            "  Downloading sister-0.2.2-py3-none-any.whl (5.3 kB)\n",
            "Collecting fasttext==0.9.2\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Janome==0.3.10\n",
            "  Downloading Janome-0.3.10-py2.py3-none-any.whl (21.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.19.0\n",
            "  Downloading numpy-1.19.0.zip (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting progressbar<3.0,>=2.5\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==2.11.0\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m674.8/674.8 KB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sister\n",
            "  Downloading sister-0.2.1-py3-none-any.whl (4.7 kB)\n",
            "  Downloading sister-0.1.10.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sister) (1.22.4)\n",
            "Collecting Janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from sister) (3.6.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sister) (1.2.0)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from fasttext==0.9.2->sister) (57.4.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from gensim->sister) (6.3.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim->sister) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim->sister) (1.10.1)\n",
            "Building wheels for collected packages: sister, fasttext\n",
            "  Building wheel for sister (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sister: filename=sister-0.1.10-py3-none-any.whl size=5394 sha256=b27d2670f477adcb4af60638971248377c7ec80835274012d668cfefa4dfb741\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/f7/2f/06675505d67efbb8c9355a68c97f02c2e8b0267469ee62bc6d\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=4391909 sha256=509e2268773d1b3cb2d6542cd59970a01815a849fbf35503115dd7b7c572124d\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
            "Successfully built sister fasttext\n",
            "Installing collected packages: Janome, pybind11, fasttext, sister\n",
            "Successfully installed Janome-0.4.2 fasttext-0.9.2 pybind11-2.10.3 sister-0.1.10\n"
          ]
        }
      ],
      "source": [
        "pip install sister"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sister\n",
        "#import gensim\n",
        "\n",
        "# Create pre-trained Word2Vec model using SISTER library\n",
        "word_embedding = sister.MeanEmbedding(lang=\"en\")\n",
        "\n",
        "\n",
        "\n",
        "# Save pre-trained Word2Vec model\n",
        "#word_embedding.save(\"word2vec.model\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a5nn-ZcKUKX",
        "outputId": "5c50f0d8-c89e-4895-a16e-d554f673ca61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def get_embedding(query,model):\n",
        "  #wordList = query.split()\n",
        "  #total = np.zeros(768)\n",
        "  # for word in wordList:\n",
        "  #   total = total + bert_embedding(word)\n",
        "  average = word_embedding(query)\n",
        "  return average"
      ],
      "metadata": {
        "id": "QcOrhDHeKiWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def get_score(document,query,model):\n",
        "  query_emb = get_embedding(query,model)\n",
        "  document_emb = get_embedding(document,model)\n",
        "  similarity = cosine_similarity(query_emb.reshape(1, -1), document_emb.reshape(1, -1))\n",
        "  return similarity\n"
      ],
      "metadata": {
        "id": "mbqjnwR4Kj3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateAllScores(model):\n",
        "  df_qrel = pd.DataFrame(columns=['query_id','doc_id','score','relevance'])\n",
        "  for item in dataset.qrels_iter():\n",
        "    df_qrel.loc[len(df_qrel.index)] = [item[0], item[1], 0,item[2]] \n",
        "  \n",
        "  for item in df_qrel.iterrows():\n",
        "    try:\n",
        "     s_doc = ' '.join((((df_docs.loc[df_docs['doc_id'] == item[1].doc_id]).processed_text).tolist())[0])\n",
        "     s_query = ' '.join((((df_q.loc[df_q['q_id'] == item[1].query_id]).processed_text).tolist())[0])\n",
        "     item[1].score = get_score(s_doc,s_query,model)[0][0]\n",
        "     #print(item[1].query_id)\n",
        "    except:\n",
        "      continue\n",
        "  return df_qrel"
      ],
      "metadata": {
        "id": "pNFFrNVFTAIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sortScores(model):\n",
        "  df_qrel = calculateAllScores(model)\n",
        "  df_qrel['query_id'] = pd.to_numeric(df_qrel['query_id'])\n",
        "  df_qrel['doc_id'] = pd.to_numeric(df_qrel['doc_id'])\n",
        "  \n",
        "  \n",
        "  df_sorted = df_qrel.sort_values(by=['query_id', 'score'], ascending=[True, False])\n",
        "  return df_sorted"
      ],
      "metadata": {
        "id": "YqVRnsk9TD_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def DCG(model):\n",
        "  df_sorted = sortScores(model)\n",
        "  unique_queries = df_sorted['query_id'].unique()\n",
        "  original_queries = (pd.to_numeric(df_q['q_id'])).unique()\n",
        "  listOfQueries = list(set(unique_queries) & set(original_queries))\n",
        "  \n",
        "  mean_DCG = 0\n",
        "  for item in listOfQueries:\n",
        "    docs = df_sorted.loc[df_sorted['query_id'] == item]\n",
        "    #print(docs)\n",
        "    DCG = 0\n",
        "    k = len(docs)\n",
        "    for i in range(1,k+1):\n",
        "      DCG += ((2 ** (docs.iloc[i-1].relevance)) / math.log(i+1, 2))\n",
        "    mean_DCG += DCG\n",
        "  mean_DCG /= len(unique_queries)\n",
        "  #print(mean_DCG)\n",
        "  return (mean_DCG)"
      ],
      "metadata": {
        "id": "6Ou_xj6FTHFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DCG for pretrained Word2Vec: \",DCG(word_embedding))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH-l2axlTHP-",
        "outputId": "c63e06a9-e30b-42d2-9e38-824ab9a124bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DCG for pretrained Word2Vec:  18.459023460459274\n"
          ]
        }
      ]
    }
  ]
}