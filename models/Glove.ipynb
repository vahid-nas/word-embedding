{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab393e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.8\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf5ce82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove-python-binary\n",
      "  Downloading glove_python_binary-0.2.0-cp38-cp38-win_amd64.whl (244 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\nas\\anaconda3\\lib\\site-packages (from glove-python-binary) (1.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\nas\\anaconda3\\lib\\site-packages (from glove-python-binary) (1.20.1)\n",
      "Installing collected packages: glove-python-binary\n",
      "Successfully installed glove-python-binary-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install glove-python-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e8d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfafc766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9d83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load('cranfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45dfab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\NAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download necessary nltk resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756aaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords: bool):\n",
    "    # Convert text to lower case\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if c.isalnum() or c.isspace())\n",
    "\n",
    "    # Remove punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Stem tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    #preprocessed_text = ' '.join(stemmed_tokens)\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b78772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of the DataFrame:\n",
      "  q_id                                     processed_text\n",
      "0    1  [similar, law, must, obey, construct, aeroelas...\n",
      "1    2  [structur, aeroelast, problem, associ, flight,...\n",
      "2    4  [problem, heat, conduct, composit, slab, solv,...\n",
      "3    8  [criterion, develop, show, empir, valid, flow,...\n",
      "4    9  [chemic, kinet, system, applic, hyperson, aero...\n",
      "5   10  [theoret, experiment, guid, turbul, couett, fl...\n",
      "6   12  [possibl, relat, avail, pressur, distribut, og...\n",
      "7   13  [method, dash, exact, approxim, dash, present,...\n",
      "8   15  [paper, intern, slip, flow, heat, transfer, st...\n",
      "9   18  [realga, transport, properti, air, avail, wide...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_docs = pd.DataFrame(columns=['doc_id', 'processed_text'])\n",
    "df_q = pd.DataFrame(columns=['q_id', 'processed_text'])\n",
    "for item in dataset.docs_iter():\n",
    "  proc1 = preprocess_text(item[1], remove_stopwords=True)\n",
    "  proc2 = preprocess_text(item[2], remove_stopwords=True)\n",
    "  #df.loc[len(df.index)] = [item[0], [preprocess_text(item[1], remove_stopwords=True), preprocess_text(item[2], remove_stopwords=True)]] \n",
    "  proc1.extend(proc2)\n",
    "  df_docs.loc[len(df_docs.index)] = [item[0], proc1]\n",
    "for item in dataset.queries_iter():\n",
    "  df_q.loc[len(df_q.index)] = [item[0], preprocess_text(item[1], remove_stopwords=True)] \n",
    "result = df_q.head(10)\n",
    "print(\"First 10 rows of the DataFrame:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcbf163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "#Creating a corpus object\n",
    "corpus = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0475d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = Glove(no_components=100, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "264f5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts1 = df_docs.processed_text.tolist()\n",
    "texts2 = df_q.processed_text.tolist()\n",
    "texts1.extend(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716560a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in texts1:\n",
    "    item = ' '.join(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e427e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.fit(texts1, window=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784f4e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n"
     ]
    }
   ],
   "source": [
    "glove = Glove(no_components=100, learning_rate=0.05) \n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c9d4f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00323711  0.02157388  0.02013845  0.05786494  0.02666519 -0.05733531\n",
      "  0.01794501 -0.0173818   0.00369853 -0.02493317  0.03123947  0.01160073\n",
      "  0.00741265 -0.0371027   0.07284716  0.03153904 -0.05933648  0.00953027\n",
      " -0.06518831 -0.02914302  0.04494675  0.01566875 -0.03119379 -0.00547448\n",
      "  0.0258231  -0.06966225 -0.01717727  0.04970566 -0.04653273 -0.01066145\n",
      "  0.08944519  0.06836338  0.06253167 -0.00469795  0.03439438 -0.05933104\n",
      "  0.03320623  0.02144831 -0.00131006  0.00375011  0.02226873  0.00848228\n",
      "  0.04017506 -0.09067384 -0.04334787 -0.01733106 -0.08960709 -0.04704098\n",
      "  0.03996418  0.02780876 -0.0347527   0.03649128 -0.02689545 -0.0345197\n",
      "  0.02025353 -0.03379716 -0.04788756 -0.05448413  0.06855027  0.00630814\n",
      " -0.02884657 -0.03271238  0.02273263 -0.00884147  0.00305772  0.08825389\n",
      "  0.04358397  0.03927837 -0.04344946 -0.02475672  0.00704874  0.05484252\n",
      "  0.08617181 -0.00557333 -0.03564884 -0.0018755  -0.0265557   0.00471218\n",
      "  0.05913074  0.04606015  0.00658584 -0.04712975 -0.02417068 -0.04721621\n",
      "  0.01983275 -0.02811869  0.00986604 -0.01254794 -0.0504564   0.02035351\n",
      " -0.05750347  0.04407379  0.00788705  0.01822024 -0.07677806 -0.00755893\n",
      " -0.01054574  0.0083086   0.02237693  0.01008156]\n"
     ]
    }
   ],
   "source": [
    "word = 'obey'\n",
    "vector = glove.word_vectors[glove.dictionary[word]]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6860e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_embedding(query,model):\n",
    "  #print(query)\n",
    "  wordList = query.split()\n",
    "  #print(wordList)\n",
    "  total = np.zeros(100)\n",
    "  for word in wordList:\n",
    "    total = total + glove.dictionary[word]\n",
    "  average = total / len(wordList)\n",
    "  return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ed65c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_calc(vec_1,vec_2):\n",
    "\n",
    "   sim = np.dot(vec_1,vec_2)/(np.linalg.norm(vec_1)*np.linalg.norm(vec_2))\n",
    "\n",
    "   return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b99d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity2(a, b):\n",
    "    nominator = np.dot(a, b)\n",
    "    \n",
    "    a_norm = np.sqrt(np.sum(a**2))\n",
    "    b_norm = np.sqrt(np.sum(b**2))\n",
    "    \n",
    "    denominator = a_norm * b_norm\n",
    "    \n",
    "    cosine_similarity = nominator / denominator\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd955d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_score(document,query,model):\n",
    "  query_emb = get_embedding(query,model)\n",
    "  document_emb = get_embedding(document,model)\n",
    "  similarity = cosine_similarity(query_emb.reshape(1, -1), document_emb.reshape(1, -1))\n",
    "  #similarity = cosine_similarity2(query_emb, document_emb)\n",
    "  return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbd8b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAllScores(model):\n",
    "  df_qrel = pd.DataFrame(columns=['query_id','doc_id','score','relevance'])\n",
    "  for item in dataset.qrels_iter():\n",
    "    df_qrel.loc[len(df_qrel.index)] = [item[0], item[1], 0,item[2]] \n",
    "  \n",
    "  for item in df_qrel.iterrows():\n",
    "    try:\n",
    "     s_doc = ' '.join((((df_docs.loc[df_docs['doc_id'] == item[1].doc_id]).processed_text).tolist())[0])\n",
    "     s_query = ' '.join((((df_q.loc[df_q['q_id'] == item[1].query_id]).processed_text).tolist())[0])\n",
    "     item[1].score = get_score(s_doc,s_query,model)[0][0]\n",
    "     #print(item[1].score)\n",
    "    except:\n",
    "      #print(\"exception\")\n",
    "      continue\n",
    "  return df_qrel\n",
    "\n",
    "#df_qrel.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f4b195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortScores(model):\n",
    "  df_qrel = calculateAllScores(model)\n",
    "  df_qrel['query_id'] = pd.to_numeric(df_qrel['query_id'])\n",
    "  df_qrel['doc_id'] = pd.to_numeric(df_qrel['doc_id'])\n",
    "  df_qrel.head(200)\n",
    "  \n",
    "  df_sorted = df_qrel.sort_values(by=['query_id', 'score'], ascending=[True, False])\n",
    "  return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3090c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def DCG(model):\n",
    "  df_sorted = sortScores(model)\n",
    "  unique_queries = df_sorted['query_id'].unique()\n",
    "  original_queries = (pd.to_numeric(df_q['q_id'])).unique()\n",
    "  listOfQueries = list(set(unique_queries) & set(original_queries))\n",
    "  \n",
    "  mean_DCG = 0\n",
    "  for item in listOfQueries:\n",
    "    docs = df_sorted.loc[df_sorted['query_id'] == item]\n",
    "    #print(docs)\n",
    "    DCG = 0\n",
    "    k = len(docs)\n",
    "    for i in range(1,k+1):\n",
    "      DCG += ((2 ** (docs.iloc[i-1].relevance)) / math.log(i+1, 2))\n",
    "    mean_DCG += DCG\n",
    "  mean_DCG /= len(unique_queries)\n",
    "  #print(mean_DCG)\n",
    "  return (mean_DCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce375686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.14683159716936"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DCG(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214c0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
